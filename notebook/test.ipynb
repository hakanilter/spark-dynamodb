{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /private/tmp/derby/metastore_db /tmp/derby /tmp/spark-warehouse/\n",
    "%pip install sparksql-magic\n",
    "%load_ext sparksql_magic\n",
    "!echo $AWS_PROFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_version = \"3.2.3\"\n",
    "hadoop_version = \"3.3.1\"\n",
    "scala_version = \"2.12\"\n",
    "\n",
    "packages = \",\".join([\n",
    "    f\"org.apache.hadoop:hadoop-aws:{hadoop_version}\"\n",
    "])\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"AnalysisTool\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/tmp/spark-warehouse\") \\\n",
    "    .config(\"spark.hadoop.fs.AbstractFileSystem.s3.impl\", \"org.apache.hadoop.fs.s3a.S3A\") \\\n",
    "    .config(\"spark.hadoop.fs.AbstractFileSystem.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3A\") \\\n",
    "    .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.profile.ProfileCredentialsProvider\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.multipart.size\", \"104857600\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Dcom.amazonaws.services.s3.enableV4=true -Dderby.system.home=/tmp/derby/xyz\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Dcom.amazonaws.services.s3.enableV4=true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.speculation\", \"false\") \\\n",
    "    .config(\"spark.jars.packages\", packages) \\\n",
    "    .config(\"spark.hive.serialization.extend.nesting.levels\", \"true\") \\\n",
    "    .config(\"spark.sql.caseSensitive\", True) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Other hadoop configs\n",
    "hadoop_config = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoop_config.set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "hadoop_config.set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\")\n",
    "hadoop_config.set(\"parquet.enable.summary-metadata\", \"false\")\n",
    "hadoop_config.set(\"dfs.client.read.shortcircuit.skip.checksum\", \"true\")\n",
    "hadoop_config.set(\"spark.hive.serialization.extend.nesting.levels\", \"true\")\n",
    "\n",
    "sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read from DynamoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "\n",
    "# This is not the ideal method but it's the easiest way. Might get an OOM.\n",
    "def deserialize(table_name):\n",
    "    client = boto3.client(service_name=\"dynamodb\", region_name=\"eu-west-1\")\n",
    "    response = client.scan(TableName=table_name)\n",
    "    data = response[\"Items\"]\n",
    "\n",
    "    # Below loop will actually give clear output from the data \n",
    "    list_data = []\n",
    "    for item in data:\n",
    "        output_dict = {}\n",
    "        for key, value in item.items():\n",
    "            if isinstance(value, dict):\n",
    "                if 'S' in value:\n",
    "                    output_dict[key] = value['S']\n",
    "                elif 'N' in value:\n",
    "                    output_dict[key] = value['N']\n",
    "        list_data.append(output_dict)\n",
    "    \n",
    "    return spark.createDataFrame(list_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the tables\n",
    "tables = [\"customer\", \"product\", \"order\"]\n",
    "for table in tables:\n",
    "    deserialize(table).createOrReplaceTempView(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "SELECT * FROM customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "SELECT * FROM product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "SELECT * FROM order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "WITH dataset AS (\n",
    "    SELECT\n",
    "        o.id AS order_id,\n",
    "        c.id AS customer_id,\n",
    "        c.name AS customer_name,\n",
    "        p.id AS product_id,\n",
    "        p.name AS product_name,\n",
    "        CAST(o.quantity AS BIGINT) AS quantity,\n",
    "        CAST(o.total AS DECIMAL(10,2)) AS total\n",
    "    FROM\n",
    "        order o\n",
    "    LEFT JOIN\n",
    "        customer c ON c.id = o.customer_id\n",
    "    LEFT JOIN\n",
    "        product p ON p.id = o.product_id\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    product_id,\n",
    "    product_name,\n",
    "    SUM(quantity) AS quantity,\n",
    "    SUM(total) AS total,\n",
    "    COUNT(DISTINCT order_id) AS num_orders\n",
    "FROM \n",
    "    dataset\n",
    "GROUP BY\n",
    "    product_id,\n",
    "    product_name\n",
    "ORDER BY\n",
    "    num_orders DESC\n",
    "LIMIT \n",
    "    10\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
